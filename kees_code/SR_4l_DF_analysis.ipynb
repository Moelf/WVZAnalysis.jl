{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.errors import InvalidArgumentError\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow GPU settings\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)#per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "from atlasify import atlasify\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_VVZ_RD.arrow')\n",
    "sig['is_signal'] = True\n",
    "# sig = sig[sig.SR == 2]\n",
    "\n",
    "bg = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_FULLBG_RD.arrow')\n",
    "bg['is_signal'] = False\n",
    "# bg = bg[bg.SR == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_raw = sorted([f for f in sig.columns if f not in ['index', 'wgt', 'is_signal', \n",
    "                                                              'Zcand_mass', 'chisq']])\n",
    "\n",
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training features\n",
    "X = pd.concat([sig[train_feats_raw], bg[train_feats_raw]], ignore_index=True)\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "for df in [sig, bg]:\n",
    "    df[train_feats_raw] = min_max_scaler.transform(df[train_feats_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-background models\n",
    "models_dir = 'models/background_id_models/'\n",
    "\n",
    "background_classifiers = {'ZZ': 1, 'Zjets': 2, 'WZ': 1, 'ttZ': 0, 'other': 1}\n",
    "\n",
    "for bc_name in background_classifiers:\n",
    "    bc_index = background_classifiers[bc_name]\n",
    "    \n",
    "    classifier = keras.models.load_model((models_dir + 'classifier_' + bc_name \n",
    "                                          + '_train_feat_test_' + str(bc_index)))\n",
    "    sig['classifier_' + bc_name + '_score'] = classifier.predict(sig[train_feat_sets[bc_index]], \n",
    "                                                                    batch_size=10000)\n",
    "    bg['classifier_' + bc_name + '_score'] = classifier.predict(bg[train_feat_sets[bc_index]], \n",
    "                                                                   batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut to 4l-DF signal region\n",
    "bg = bg[bg.SR == 1]\n",
    "sig = sig[sig.SR == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, b, _ = plt.hist(bg.classifier_ZZ_score, bins=100, weights=bg.wgt, density=True, alpha=0.5)\n",
    "plt.hist(sig.classifier_ZZ_score, bins=b, weights=sig.wgt, density=True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, b, _ = plt.hist(bg.classifier_Zjets_score, bins=100, weights=bg.wgt, density=True, alpha=0.5)\n",
    "plt.hist(sig.classifier_Zjets_score, bins=b, weights=sig.wgt, density=True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, b, _ = plt.hist(bg.classifier_WZ_score, bins=100, weights=bg.wgt, density=True, alpha=0.5)\n",
    "plt.hist(sig.classifier_WZ_score, bins=b, weights=sig.wgt, density=True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, b, _ = plt.hist(bg.classifier_ttZ_score, bins=100, weights=bg.wgt, density=True, alpha=0.5)\n",
    "plt.hist(sig.classifier_ttZ_score, bins=b, weights=sig.wgt, density=True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, b, _ = plt.hist(bg.classifier_other_score, bins=100, weights=bg.wgt, density=True, alpha=0.5)\n",
    "plt.hist(sig.classifier_other_score, bins=b, weights=sig.wgt, density=True, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scan model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_sig(s, b):\n",
    "    if s == 0:\n",
    "        return 0\n",
    "    return np.sqrt(2 * ((s + b) * np.log(1 + s / b) - s))\n",
    "\n",
    "n_bg = sum(bg.wgt)\n",
    "n_sig = sum(sig.wgt)\n",
    "\n",
    "print('There are', n_bg, 'background events')\n",
    "print('There are', n_sig, 'signal events')\n",
    "print('')\n",
    "print('S/B =', n_sig/n_bg)\n",
    "print('Starting significance is', region_sig(n_sig, n_bg), 'sigma')\n",
    "print('Corresponds to', np.sqrt(2.0) * region_sig(n_sig, n_bg), 'sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_classifier_score_feats = ['classifier_' + bc + '_score' for bc in background_classifiers]\n",
    "combined_train_feats_raw = train_feats_raw + bg_classifier_score_feats\n",
    "\n",
    "combined_train_feat_sets = [combined_train_feats_raw, \n",
    "                            [f for f in combined_train_feats_raw if f not in bg_classifier_score_feats],\n",
    "                            [f for f in combined_train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', \n",
    "                                                                              'Zlep1_phi', 'Zlep2_phi']],\n",
    "                            [f for f in combined_train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                            [f for f in combined_train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                            [f for f in combined_train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[combined_train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(bg[combined_train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[combined_train_feats_raw], bg_test[combined_train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = 'SR_4l_DF_models/'\n",
    "model_dir = 'models/' + dir_base\n",
    "plot_dir = 'plots/' + dir_base\n",
    "model_name_group = 'classifier_train_feat_test'\n",
    "\n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_tests = [model_dir + t for t in os.listdir(model_dir) \n",
    "                   if t.startswith(model_name_group) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[combined_train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[combined_train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[combined_train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('4$\\ell$-DF', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig(plot_dir + model_name_group + '.png', pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig(plot_dir + model_name_group + '.pdf', pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training history plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    history_plot_dir = plot_dir + 'train_feat_test/'\n",
    "    if not os.path.exists(history_plot_dir):\n",
    "        os.makedirs(history_plot_dir)\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('4$\\ell$-DF Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig(history_plot_dir + 'test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig(history_plot_dir + '/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('4$\\ell$-DF Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig(history_plot_dir + '/test_' + str(i) + '_acc.png', pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig(history_plot_dir + '/test_' + str(i) + '_acc.pdf', pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate signal vs background histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=40, weights=bg_test.wgt, label='Bg', \n",
    "                       density=False, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=False, alpha=0.5)\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Events', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('4$\\ell$-DF Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig(history_plot_dir + 'test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig(history_plot_dir + 'test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate significance scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_i, test in enumerate(completed_tests):\n",
    "    # Perform significance scan\n",
    "    n_sig_pts = 100\n",
    "\n",
    "    nn_cuts = np.arange(0.0, 1.0, 1.0/n_sig_pts)\n",
    "    significances = [float('nan')]*n_sig_pts\n",
    "\n",
    "    max_sig_loc = 0\n",
    "    max_sig = 0\n",
    "\n",
    "    for i, nn_cut in enumerate(nn_cuts):\n",
    "        n_bg = sum(bg_test[bg_test['output_test_' + str(test_i)] > nn_cut].wgt)\n",
    "        n_sig = sum(sig_test[sig_test['output_test_' + str(test_i)] > nn_cut].wgt)\n",
    "\n",
    "        try:\n",
    "            current_sig = region_sig(n_sig, n_bg)\n",
    "        except ZeroDivisionError:\n",
    "            current_sig = float('nan')\n",
    "        significances[i] = current_sig\n",
    "\n",
    "        if current_sig > max_sig:\n",
    "            max_sig = current_sig\n",
    "            max_sig_loc = nn_cut\n",
    "\n",
    "    print('Test ' + str(test_i))\n",
    "    print('Max observed significance:', max_sig, 'sigma at cut of', max_sig_loc)\n",
    "    print('Corresponds to', 2*max_sig, 'sigma with full dataset')\n",
    "    \n",
    "    # Plot scan\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(nn_cuts, significances)\n",
    "\n",
    "    plt.axhline(y=max_sig, color='black', ls='--')\n",
    "    plt.axvline(x=max_sig_loc, color='black', ls='--')\n",
    "\n",
    "    plt.ylabel('Significance', fontsize=14)\n",
    "    plt.xlabel('NN cut', fontsize=14)\n",
    "    plt.title('4$\\ell$-DF Test ' + str(test_i), loc='right', fontsize=14)\n",
    "\n",
    "    plt.text(min(nn_cuts), min(significances), \n",
    "             'Max: %.2f $\\sigma$\\nCorresponds to %.2f $\\sigma$\\nLoc: %.2f'%(max_sig, 2*max_sig, max_sig_loc),\n",
    "             fontsize=14)\n",
    "\n",
    "    atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "    local_plot_name = 'sig_scan_test_' + str(test_i)\n",
    "    plt.savefig(plot_dir + local_plot_name + '.png')\n",
    "    plt.savefig(plot_dir + local_plot_name + '.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined significance scan\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "max_sig_loc = 0\n",
    "max_sig_index = 0\n",
    "max_sig = 0\n",
    "\n",
    "for test_i, test in enumerate(completed_tests):\n",
    "    # Perform significance scan\n",
    "    n_sig_pts = 100\n",
    "\n",
    "    nn_cuts = np.arange(0.0, 1.0, 1.0/n_sig_pts)\n",
    "    significances = [float('nan')]*n_sig_pts\n",
    "\n",
    "    for i, nn_cut in enumerate(nn_cuts):\n",
    "        n_bg = sum(bg_test[bg_test['output_test_' + str(test_i)] > nn_cut].wgt)\n",
    "        n_sig = sum(sig_test[sig_test['output_test_' + str(test_i)] > nn_cut].wgt)\n",
    "\n",
    "        try:\n",
    "            current_sig = region_sig(n_sig, n_bg)\n",
    "        except ZeroDivisionError:\n",
    "            current_sig = float('nan')\n",
    "        significances[i] = current_sig\n",
    "\n",
    "        if current_sig > max_sig:\n",
    "            max_sig = current_sig\n",
    "            max_sig_loc = nn_cut\n",
    "            max_sig_index = test_i\n",
    "\n",
    "    print('Test ' + str(test_i))\n",
    "    print('Max observed significance:', max_sig, 'sigma at cut of', max_sig_loc)\n",
    "    print('Corresponds to', 2*max_sig, 'sigma with full dataset')\n",
    "    \n",
    "    # Plot scan\n",
    "    plt.plot(nn_cuts, significances, label='Test ' + str(test_i))\n",
    "\n",
    "plt.axhline(y=max_sig, color='black', ls='--')\n",
    "plt.axvline(x=max_sig_loc, color='black', ls='--')\n",
    "\n",
    "plt.ylabel('Significance', fontsize=14)\n",
    "plt.xlabel('NN cut', fontsize=14)\n",
    "plt.title('4$\\ell$-DF', loc='right', fontsize=14)\n",
    "\n",
    "plt.text(min(nn_cuts), min(significances), \n",
    "         ('Max: %.2f $\\sigma$\\nCorresponds to %.2f $\\sigma$\\nLoc: %.2f'%(max_sig, 2*max_sig, max_sig_loc)\n",
    "          +'\\nTest index: %i'%(max_sig_index)),\n",
    "         fontsize=14)\n",
    "\n",
    "lgd = plt.legend(fontsize=14, bbox_to_anchor=(1.0, 1.0))\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "local_plot_name = 'sig_scan_combined'\n",
    "plt.savefig(plot_dir + local_plot_name + '.png', bbox_extra_artists=(lgd,), bbox_inches='tight')\n",
    "plt.savefig(plot_dir + local_plot_name + '.pdf', bbox_extra_artists=(lgd,), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wvz_machine_learning",
   "language": "python",
   "name": "wvz_machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
