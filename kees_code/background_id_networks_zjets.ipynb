{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.errors import InvalidArgumentError\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow GPU settings\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)#per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "from atlasify import atlasify\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_VVZ_RD.arrow')\n",
    "sig['is_signal'] = True\n",
    "# sig_test = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/SIG_2021_11_16_no_iso_TEST.arrow')\n",
    "bg_full = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_FULLBG_RD.arrow')\n",
    "bg_full['is_signal'] = False\n",
    "# bg_test = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/FULLBG_2021_11_16_no_iso_TEST.arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_ZZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_ZZ_RD.arrow')\n",
    "bg_Zjets_old = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20211129_iso_e4m1_Zjets_RD.arrow')\n",
    "bg_Zjets = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_Zjets_RD.arrow')\n",
    "bg_Zgamma = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_Zgamma_RD.arrow')\n",
    "bg_WZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_WZ_RD.arrow')\n",
    "bg_tZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_tZ_RD.arrow')\n",
    "bg_tWZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_tWZ_RD.arrow')\n",
    "bg_ttZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_ttZ_RD.arrow')\n",
    "bg_other = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_others_RD.arrow')\n",
    "\n",
    "bg_sources = [bg_ZZ, bg_Zjets, bg_Zgamma, bg_WZ, bg_tZ, bg_tWZ, bg_ttZ, bg_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in bg_sources:\n",
    "    print(sum(df.wgt) / sum(bg_full.wgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bg_Zjets_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bg_Zjets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = 'Zcand_mass'\n",
    "\n",
    "\n",
    "bg_source = bg_Zjets\n",
    "\n",
    "plt.hist(bg_source[feat], weights=bg_source.wgt, bins=50)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel(feat, fontsize=12)\n",
    "plt.ylabel('Weighted events', fontsize=12)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "_, b, _ = plt.hist(bg_source[feat], weights=bg_source.wgt, density=False, bins=50, alpha=0.5, label='Bkg')\n",
    "plt.hist(sig[feat], weights=sig.wgt, density=False, bins=b, alpha=0.5, label='Sig')\n",
    "\n",
    "plt.legend(frameon=False, fontsize=12)\n",
    "\n",
    "plt.xlabel(feat, fontsize=12)\n",
    "plt.ylabel('Weighted density', fontsize=12)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feats = ['Zlep1_dphi', 'Zlep2_dphi', 'Wlep1_dphi', 'Wlep2_dphi', 'pt_1', 'pt_2', 'Wlep2_pt_sqrt']\n",
    "# train_feats = ['pt_1', 'pt_2', 'pt_3', 'pt_4', 'pt_4l',\n",
    "#                'Zlep1_dphi', 'Zlep2_dphi', 'Wlep1_dphi', 'Wlep2_dphi', \n",
    "#                'Zlep1_phi', 'Zlep2_phi', 'Wlep1_phi', 'Wlep2_phi', \n",
    "#                'Zlep1_eta', 'Zlep2_eta', 'Wlep1_eta', 'Wlep2_eta',\n",
    "#                'Zlep1_pid', 'Zlep2_pid', 'Wlep1_pid', 'Wlep2_pid',\n",
    "#                'Zlep1_pt', 'Zlep2_pt', 'Wlep1_pt', 'Wlep2_pt',\n",
    "#                'METSig', 'MET', 'Nlep', 'Njet',\n",
    "#                'other_mass', 'leptonic_HT', 'total_HT', 'HT', \n",
    "#                'SR']\n",
    "\n",
    "train_feats_raw = sorted([f for f in sig.columns if f not in ['index', 'wgt', 'is_signal', 'Zcand_mass', 'chisq']])\n",
    "\n",
    "X = pd.concat([sig[train_feats_raw], bg_full[train_feats_raw]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs for NN training\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "for df in [sig, bg_full] + bg_sources:\n",
    "    df[train_feats_raw] = min_max_scaler.transform(df[train_feats_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs Z + jets\n",
    "\n",
    "Z + jets events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = bg_Zjets\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can scan over different training setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(train_feats_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5000\n",
    "patience = 500\n",
    "batch_size = 512\n",
    "num_nodes = 32\n",
    "dropout = 0.1\n",
    "learn_rate = 1e-5\n",
    "\n",
    "for i, train_feats in enumerate(train_feat_sets):\n",
    "    print('Running with training features:', train_feats)\n",
    "    # Save training setup\n",
    "    with open('models/background_id_models/classifier_Zjets_train_feat_test_' + str(i) + '_setup.txt', \n",
    "              'w') as file:\n",
    "        file.write('Epochs: ' + str(EPOCHS) + '\\n')\n",
    "        file.write('Patience: ' + str(patience) + '\\n')\n",
    "        file.write('Learning rate: ' + str(learn_rate) + '\\n')\n",
    "        file.write('Batch size: ' + str(batch_size) + '\\n\\n')\n",
    "        file.write('Training features:\\n' + '\\n'.join(train_feats))\n",
    "    \n",
    "    # Generate train and test samples\n",
    "    sig_train, sig_test = train_test_split(sig[train_feats + ['wgt']], train_size=0.5, random_state=314)\n",
    "    bg_train, bg_test = train_test_split(current_bg[train_feats + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "    n_sig = sum(sig_train.wgt)\n",
    "    n_bg = sum(bg_train.wgt)\n",
    "\n",
    "    x_train_sig = sig_train[train_feats]\n",
    "    x_train_bg = bg_train[train_feats]\n",
    "\n",
    "    x_train = pd.concat([x_train_sig, x_train_bg])\n",
    "    y_train = np.concatenate([np.ones(len(sig_train)), np.zeros(len(bg_train))])\n",
    "    w_train = pd.Series(np.concatenate([(n_sig + n_bg) / n_sig * sig_train['wgt'], \n",
    "                                        (n_sig + n_bg) / n_bg * bg_train['wgt']]))\n",
    "\n",
    "    n_sig_test = sum(sig_test.wgt)\n",
    "    n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "    x_test = pd.concat([sig_test[train_feats], bg_test[train_feats]])\n",
    "    y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "    w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                       (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))\n",
    "    \n",
    "    # Generate and fit model\n",
    "    K.clear_session()\n",
    "    classifier_Zjets = Sequential()\n",
    "    classifier_Zjets.add(Dense(num_nodes, input_dim=x_train.shape[1], activation='relu')) \n",
    "    classifier_Zjets.add(Dropout(dropout))\n",
    "    classifier_Zjets.add(Dense(num_nodes, activation='relu'))\n",
    "    classifier_Zjets.add(Dropout(dropout))\n",
    "    classifier_Zjets.add(Dense(num_nodes, activation='relu'))\n",
    "    classifier_Zjets.add(Dropout(dropout))\n",
    "    classifier_Zjets.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "    classifier_Zjets.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    # Early stopping\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    history = classifier_Zjets.fit(x_train, y_train, epochs=EPOCHS, batch_size=batch_size,\n",
    "                                   validation_data=(x_test, y_test, w_test), sample_weight=w_train, \n",
    "                                   verbose=1, callbacks=[callback], shuffle=True)\n",
    "    \n",
    "    # Save model and history\n",
    "    classifier_Zjets.save('models/background_id_models/classifier_Zjets_train_feat_test_' + str(i))\n",
    "    with open('models/background_id_models/classifier_Zjets_train_feat_test_' + str(i) + '_history.pkl', \n",
    "              'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wvz_machine_learning",
   "language": "python",
   "name": "wvz_machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
