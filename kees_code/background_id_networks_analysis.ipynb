{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.errors import InvalidArgumentError\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow GPU settings\n",
    "# gpu_options = tf.GPUOptions(allow_growth=True)#per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "from atlasify import atlasify\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_old = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220117_iso_e4m1_VVZ_RD.arrow')\n",
    "\n",
    "sig = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_VVZ_RD.arrow')\n",
    "sig['is_signal'] = True\n",
    "bg_full = pd.read_feather(('/home/grabanal/WVZ/gabriel_ML_data/'\n",
    "                           + '20220301_ELReLMIs54_MUReLMIs31_btag77_FULLBG_RD.arrow'))\n",
    "bg_full['is_signal'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_ZZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_ZZ_RD.arrow')\n",
    "bg_Zjets_old = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_Zjets_RD.arrow')\n",
    "bg_Zjets = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_Zjets_RD.arrow')\n",
    "bg_Zgamma = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_Zgamma_RD.arrow')\n",
    "bg_WZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_WZ_RD.arrow')\n",
    "bg_tZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_tZ_RD.arrow')\n",
    "bg_tWZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_tWZ_RD.arrow')\n",
    "bg_ttZ = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_ttZ_RD.arrow')\n",
    "bg_other = pd.read_feather('/home/grabanal/WVZ/gabriel_ML_data/20220301_ELReLMIs54_MUReLMIs31_btag77_others_RD.arrow')\n",
    "\n",
    "bg_sources = [bg_ZZ, bg_Zjets, bg_Zgamma, bg_WZ, bg_tZ, bg_tWZ, bg_ttZ, bg_other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in bg_sources:\n",
    "    print(sum(df.wgt) / sum(bg_full.wgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feats_raw = sorted([f for f in sig_old.columns if f not in ['index', 'wgt', 'is_signal', \n",
    "                                                                  'Zcand_mass', 'chisq']])\n",
    "\n",
    "X = pd.concat([sig[train_feats_raw], bg_full[train_feats_raw]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize inputs for NN training\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "for df in [sig, bg_full] + bg_sources:\n",
    "    df[train_feats_raw] = min_max_scaler.transform(df[train_feats_raw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs ZZ\n",
    "\n",
    "ZZ events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = bg_ZZ\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(current_bg[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[train_feats_raw], bg_test[train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scan over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_str = '20220301_classifier_ZZ'\n",
    "plot_str = scan_test_str\n",
    "\n",
    "completed_tests = ['models/background_id_models/' + t for t in os.listdir('models/background_id_models/') \n",
    "                   if t.startswith(scan_test_str) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('Signal vs other', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('ZZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    if not os.path.exists('plots/background_id_models/' + plot_str + '/train_feat_test/'):\n",
    "        os.makedirs('plots/background_id_models/' + plot_str + '/train_feat_test/')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('ZZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=30, weights=bg_test.wgt, label='Other', \n",
    "                       density=True, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs Z + jets\n",
    "\n",
    "Z + jets events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = bg_Zjets\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(current_bg[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "# n_sig = sum(sig_train.wgt)\n",
    "# n_bg = sum(bg_train.wgt)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[train_feats_raw], bg_test[train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scan over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_str = '20220301_classifier_Zjets'\n",
    "plot_str = scan_test_str\n",
    "\n",
    "completed_tests = ['models/background_id_models/' + t for t in os.listdir('models/background_id_models/') \n",
    "                   if t.startswith(scan_test_str) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('Signal vs other', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Z + jets Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    if not os.path.exists('plots/background_id_models/' + plot_str + '/train_feat_test/'):\n",
    "        os.makedirs('plots/background_id_models/' + plot_str + '/train_feat_test/')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Z + jets Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=30, weights=bg_test.wgt, label='Other', \n",
    "                       density=True, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs ttZ\n",
    "\n",
    "ttZ events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = bg_ttZ\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(current_bg[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "# n_sig = sum(sig_train.wgt)\n",
    "# n_bg = sum(bg_train.wgt)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[train_feats_raw], bg_test[train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scan over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_str = '20220301_classifier_ttZ'\n",
    "plot_str = scan_test_str\n",
    "\n",
    "completed_tests = ['models/background_id_models/' + t for t in os.listdir('models/background_id_models/') \n",
    "                   if t.startswith(scan_test_str) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('Signal vs other', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('ttZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    if not os.path.exists('plots/background_id_models/' + plot_str + '/train_feat_test/'):\n",
    "        os.makedirs('plots/background_id_models/' + plot_str + '/train_feat_test/')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('ttZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=30, weights=bg_test.wgt, label='Other', \n",
    "                       density=True, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs WZ\n",
    "\n",
    "WZ events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = bg_WZ\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(current_bg[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "# n_sig = sum(sig_train.wgt)\n",
    "# n_bg = sum(bg_train.wgt)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[train_feats_raw], bg_test[train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scan over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_str = '20220301_classifier_WZ'\n",
    "plot_str = scan_test_str\n",
    "\n",
    "completed_tests = ['models/background_id_models/' + t for t in os.listdir('models/background_id_models/') \n",
    "                   if t.startswith(scan_test_str) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('Signal vs other', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('WZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    if not os.path.exists('plots/background_id_models/' + plot_str + '/train_feat_test/'):\n",
    "        os.makedirs('plots/background_id_models/' + plot_str + '/train_feat_test/')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('WZ Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=30, weights=bg_test.wgt, label='Other', \n",
    "                       density=True, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs other\n",
    "\n",
    "other events account for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_bg = pd.concat([bg_Zgamma, bg_tZ, bg_tWZ, bg_other])\n",
    "sum(current_bg.wgt) / sum(bg_full.wgt) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "percent of the total background. The amount relative to signal is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number:', len(current_bg)/len(sig) * 100, '%')\n",
    "print('Weight:', sum(current_bg.wgt)/sum(sig.wgt) * 100, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_sets = [train_feats_raw, \n",
    "                   [f for f in train_feats_raw if f not in ['Wlep1_phi', 'Wlep2_phi', 'Zlep1_phi', 'Zlep2_phi']],\n",
    "                   [f for f in train_feats_raw if f not in ['MET', 'METSig']],\n",
    "                   [f for f in train_feats_raw if f not in ['pt_1', 'pt_2', 'pt_3', 'pt_4']],\n",
    "                   [f for f in train_feats_raw if f not in ['Njet', 'Nlep']]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regenerate test samples\n",
    "_, sig_test = train_test_split(sig[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "_, bg_test = train_test_split(current_bg[train_feats_raw + ['wgt']], train_size=0.5, random_state=314)\n",
    "\n",
    "n_sig_test = sum(sig_test.wgt)\n",
    "n_bg_test = sum(bg_test.wgt)\n",
    "\n",
    "x_test = pd.concat([sig_test[train_feats_raw], bg_test[train_feats_raw]])\n",
    "y_test = np.concatenate([np.ones(len(sig_test)), np.zeros(len(bg_test))])\n",
    "w_test = pd.Series(np.concatenate([(n_sig_test + n_bg_test) / n_sig_test * sig_test['wgt'], \n",
    "                                   (n_sig_test + n_bg_test) / n_bg_test * bg_test['wgt']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we scan over tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_test_str = '20220301_classifier_other'\n",
    "plot_str = scan_test_str\n",
    "\n",
    "completed_tests = ['models/background_id_models/' + t for t in os.listdir('models/background_id_models/') \n",
    "                   if t.startswith(scan_test_str) and t.endswith('_history.pkl')]\n",
    "completed_tests = [t.replace('_history.pkl', '') for t in completed_tests]\n",
    "completed_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [None] * len(completed_tests)\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    models[i] = keras.models.load_model(test)\n",
    "    \n",
    "    x_test['output_test_' + str(i)] = models[i].predict(x_test[train_feat_sets[i]], batch_size=10000)\n",
    "    sig_test['output_test_' + str(i)] = models[i].predict(sig_test[train_feat_sets[i]], batch_size=10000)\n",
    "    bg_test['output_test_' + str(i)] = models[i].predict(bg_test[train_feat_sets[i]], batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], ls='--', color='grey', label='Random')\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    fpr, tpr, _ = roc_curve(y_test, x_test['output_test_' + str(i)], sample_weight=w_test)\n",
    "    \n",
    "    plt.plot(fpr, tpr, label='Test ' + str(i))\n",
    "    \n",
    "plt.legend(fontsize=14)\n",
    "plt.xlabel('False positive rate', fontsize=14)\n",
    "plt.ylabel('True positive rate', fontsize=14)\n",
    "plt.title('Signal vs other', fontsize=16, loc='right')\n",
    "\n",
    "atlasify('Internal Simulation', outside=True)\n",
    "\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "plt.savefig('plots/background_id_models/' + plot_str + '_train_feat_test_roc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training history plots\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    with open(test + '_history.pkl', 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loss\n",
    "    plt.plot(history['loss'], label='loss')\n",
    "    plt.plot(history['val_loss'], label='val loss')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('other Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    if not os.path.exists('plots/background_id_models/' + plot_str + '/train_feat_test/'):\n",
    "        os.makedirs('plots/background_id_models/' + plot_str + '/train_feat_test/')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_loss.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.plot(history['accuracy'], label='acc')\n",
    "    plt.plot(history['val_accuracy'], label='val acc')\n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('other Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_acc.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate signal vs background histograms\n",
    "\n",
    "for i, test in enumerate(completed_tests):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    _, b, _ = plt.hist(bg_test['output_test_' + str(i)], bins=30, weights=bg_test.wgt, label='Other', \n",
    "                       density=True, alpha=0.5)\n",
    "    plt.hist(sig_test['output_test_' + str(i)], bins=b, weights=sig_test.wgt, label='Sig', \n",
    "             density=True, alpha=0.5)\n",
    "    \n",
    "    \n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('NN output', fontsize=12)\n",
    "    plt.ylabel('Density', fontsize=12)\n",
    "    \n",
    "    atlasify('Internal simulation', outside=True)\n",
    "    \n",
    "    plt.title('Test ' + str(i), fontsize=14, loc='right')\n",
    "    \n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.png', \n",
    "            pad_inches=0.05, bbox_inches='tight')\n",
    "    plt.savefig('plots/background_id_models/' + plot_str + '/train_feat_test/test_' + str(i) + '_output.pdf', \n",
    "            pad_inches=0.05, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wvz_machine_learning",
   "language": "python",
   "name": "wvz_machine_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
